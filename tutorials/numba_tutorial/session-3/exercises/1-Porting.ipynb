{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba Session 3 exercises\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "* Object mode and loop lifting\n",
    "  * Inspection using the `numba` tool\n",
    "* Rewriting NumPy array operations for CUDA kernels using:\n",
    "  * In-kernel code\n",
    "  * CuPy\n",
    "* Asynchronous data movement and kernel launches using streams\n",
    "* Timing asynchronous operations using events\n",
    "* CUDA target utility functions:\n",
    "  * Random Number Generation\n",
    "  * Reduce Generation\n",
    "  * ForAll Generation\n",
    "* Kernel caching and timing\n",
    "* The CUDA Array Interface\n",
    "\n",
    "Debugging with the simulator and cuda-memcheck were also part of the Session 3 presentation. There are exercises involving the simulator and cuda-gdb in `2-Debug.ipynb`.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Let's first import everything required by the rest of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from numba import cuda, jit\n",
    "\n",
    "import cupy\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porting\n",
    "\n",
    "This section covers useful tools and features when porting code, which is easiest when done in a sequence of stages:\n",
    "\n",
    "* Compile functions with Object Mode (if necessary)\n",
    "* Edit Object Mode functions to compile with Nopython Mode\n",
    "* Move Nopython Mode functions to the GPU with the CUDA JIT decorator\n",
    "* Optimize data movement\n",
    "* Optimize computation with asynchronous operations\n",
    "\n",
    "### Object mode and loop lifting\n",
    "\n",
    "This is an example of a function that can only be compiled in Object Mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def hash_computation(x):\n",
    "    # Loop and body supported by Nopython mode\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] ** 2\n",
    "    # Unsupported library / function call\n",
    "    return hashlib.md5(x).digest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call it, we can expect to see warnings from Numba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(100)\n",
    "hash_computation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warnings indicate that the `md5` attribute of the `hashlib` module is unknown to Numba.\n",
    "\n",
    "Numba lifts the loop that can be compiled in Nopython Mode out of the function and calls it from the Object Mode function, so the code is compiled as if it were written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def outline_1(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] ** 2\n",
    "\n",
    "\n",
    "@jit\n",
    "def hash_computation_internal(x):\n",
    "    outline_1(x)\n",
    "    # Unsupported library / function call\n",
    "    return hashlib.md5(x).digest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call the `outline_1` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our success in calling `outline_1` confirms that it could be compiled in Nopython Mode (no warning or error is produced).\n",
    "\n",
    "Now if we call `hash_computation_internal` we should again see the warning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_computation_internal(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop Lifting inspection\n",
    "\n",
    "The `numba` command line tool can be used to produce a report that shows where and why loop lifting occurred. To demonstrate this in the notebook, we first need to write out our above example as a self-contained file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inspection_example.py\n",
    "from numba import jit\n",
    "\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "@jit\n",
    "def hash_computation(x):\n",
    "    # Loop and body supported by Nopython mode\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] ** 2\n",
    "    # Unsupported library / function call\n",
    "    return hashlib.md5(x).digest()\n",
    "\n",
    "x = np.arange(100)\n",
    "hash_computation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run `numba` over it to generate an HTML report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!numba --annotate-html=inspection_example.html inspection_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we open the report, we should see the lines that can be compiled in Nopython mode highlighted in green, and those that needed Object Mode in red.\n",
    "\n",
    "Try:\n",
    "* Clicking the triangle to the left of a source line to expand the Numba IR.\n",
    "* Look at the line that could not be compiled in Nopython mode - identify the items that are typed as `pyobject`\n",
    "  * If the code can be modified such that there are no `pyobject` types, it can then be compiled in Nopython mode.\n",
    "  * For this example, a Nopython-mode compatible implementation of an MD5 function could be substituted for the `hashlib.md5` call (though finding / implementing one may be time consuming - it is not suggested as an exercise for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(filename='inspection_example.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting NumPy array ops\n",
    "\n",
    "The following function can be used with Numba's CPU target without issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def normalize(x):\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :] = x[i, :] / x[i, :].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an array on the host and call the function on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(32 * 3).astype(np.float32).reshape((32, 3))\n",
    "normalize(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try and modify the function to run on the CUDA target. A first approach might consist of:\n",
    "\n",
    "* Replacing `@jit` with `@cuda.jit`\n",
    "* Distributing the loop across threads with `cuda.grid(1)`\n",
    "\n",
    "resulting in something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def normalize(x):\n",
    "    i = cuda.grid(1)\n",
    "    \n",
    "    x[i, :] = x[i, :] / x[i, :].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to use this implementation, we have a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(32 * 3).astype(np.float32).reshape((32, 3))\n",
    "normalize[1, 32](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba doesn't recognise the division of an array, by another value, because array operations are not supported in the CUDA target - the kernel needs re-writing to implement the array division, and also the computation of the mean, using only supported mechanisms.\n",
    "\n",
    "#### Exercises\n",
    "\n",
    "1. Rewrite the `normalize` kernel so that the array division and mean are computed using for loops and scalar computations with array indexing, so that it can be compiled by Numba.\n",
    "2. Rewrite the computation in terms of CuPy array operations instead of a Numba kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data movement with streams, and timing with events\n",
    "\n",
    "This next example demonstrates using streams to asynchronously transfer data to and from the device and launch kernels on the streamed data, as well as using events to time these asynchronous events.\n",
    "\n",
    "First we set things up by creating:\n",
    "\n",
    "* A kernel to launch\n",
    "* A stream to operate on\n",
    "* Host memory allocations using a pinned array - pinned host memory is needed for asynchronous transfers\n",
    "* Device memory allocations, for the data to be copied to\n",
    "* Events to mark the start and stop times of asynchronous execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def increment_kernel(g_data, inc_value):\n",
    "    i = cuda.grid(1)\n",
    "    g_data[i] = g_data[i] + inc_value\n",
    "\n",
    "# Create a stream - differs from CUDA C++ example\n",
    "stream = cuda.stream()\n",
    "\n",
    "# Allocate host memory\n",
    "n = 16 * 1024 * 1024\n",
    "a = cuda.pinned_array(n, dtype=np.int32)\n",
    "a[:] = 0\n",
    "\n",
    "# Allocate device memory\n",
    "d_a = cuda.device_array_like(a, stream)\n",
    "\n",
    "# Kernel configuration\n",
    "nthreads = 512\n",
    "nblocks = n // nthreads\n",
    "\n",
    "# Create event handles\n",
    "start = cuda.event()\n",
    "stop = cuda.event()\n",
    "\n",
    "value = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting everything up, we launch the kernel to ensure that compilation overhead is not part of our timing. We launch the kernel twice, with the second launch undoing the effect of the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kernel to make sure compilation happens outside the async portion\n",
    "increment_kernel[nblocks, nthreads, stream](d_a, -value)\n",
    "increment_kernel[nblocks, nthreads, stream](d_a, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can asynchronously issue work. For timing purposes, we first synchronize to ensure that there are no other pending operations when we record the start time. Next, we asynchronously transfer data to the device, launch the kernel, and transfer data back.\n",
    "\n",
    "The `time()` function is used to record CPU timings; the GPU timings are recorded using the events.\n",
    "\n",
    "Because all asynchronous operations return immediately, we spin the CPU until all asynchronous operations have completed, by checking for completion with `stop.query()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Asynchronously issue work\n",
    "cuda.synchronize()\n",
    "start_time = time.time()\n",
    "start.record(stream=stream)\n",
    "d_a.copy_to_device(a, stream=stream)\n",
    "increment_kernel[nblocks, nthreads, stream](d_a, value)\n",
    "d_a.copy_to_host(a)\n",
    "stop.record(stream=stream)\n",
    "stop_time = time.time()\n",
    "\n",
    "# Stage 2: Have CPU do some work while waiting for stage 1 to finish\n",
    "counter = 0\n",
    "while not stop.query():\n",
    "    counter += 1\n",
    "\n",
    "gpu_time = start.elapsed_time(stop)\n",
    "cpu_time = (stop_time - start_time) * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the time spent by the GPU, and the time the CPU spent in CUDA calls. We should be able to see that the CPU was able to do some other work whilst the GPU was executing asynchronously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time spent executing by the GPU: %.2f\" % gpu_time)\n",
    "print(\"time spent by CPU in CUDA calls: %.2f\" % cpu_time)\n",
    "print(\"CPU executed %lu iterations while waiting for GPU to finish\" %\n",
    "      counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Number Generation\n",
    "\n",
    "This section provides a complete example of the use of the CUDA random number generator, by using it to probabilistically compute pi.\n",
    "\n",
    "First we import functions to initialise the RNG state, and a function to draw random numbers from the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define the kernel. The pertinent parts of this example are:\n",
    "* The random number generator states need to be passed into the kernel - here, in `rng_states`.\n",
    "* The `xoroshiro128p_uniform_float32` function is used to draw a random float32 from a uniform distribution. There are other functions that follow a similar naming convention - for a full list, the [CUDA Random Number Generator documentation](https://numba.pydata.org/numba-doc/latest/cuda/random.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def compute_pi(rng_states, iterations, out):\n",
    "    \"\"\"Find the maximum value in values and store in result[0]\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "\n",
    "    # Compute pi by drawing random (x, y) points and finding what\n",
    "    # fraction lie inside a unit circle\n",
    "    inside = 0\n",
    "    for i in range(iterations):\n",
    "        x = xoroshiro128p_uniform_float32(rng_states, tid)\n",
    "        y = xoroshiro128p_uniform_float32(rng_states, tid)\n",
    "        if x**2 + y**2 <= 1.0:\n",
    "            inside += 1\n",
    "\n",
    "    out[tid] = 4.0 * inside / iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to using the RNG, we need to initialize its states. The state size should be equal to the total number of threads in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nthreads = 64\n",
    "nblocks = 24    \n",
    "state_size = nblocks * nthreads\n",
    "rng_states = create_xoroshiro128p_states(state_size, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the kernel using our initialized states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.zeros(nthreads * nblocks, dtype=np.float32)\n",
    "compute_pi[nblocks, nthreads](rng_states, 10000, out)\n",
    "print('pi:', out.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reduce Generator\n",
    "\n",
    "The reduction generator simplifies the generation of fast reduction kernels. It accepts a function defined in terms of a pair of elements to reduce, and generates a kernel that applies the reduction to all elements of an input array. We can create a reduction using the `@cuda.reduce decorator` - here we will create a simple sum-reduction, but more complex computations are possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.reduce\n",
    "def sum_reduce(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an array to test the reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = (np.arange(1234, dtype=np.float64)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call the reduction function on an array residing on the host, it automatically transfers data to the device, and transfers the result back to return it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_reduce(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to keep the result on the device, we need a device array to store the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cuda.device_array(1, dtype=np.float64)\n",
    "sum_reduce(A, res=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling a reduction kernel on a device array does not overwrite the device array (the documentation presently states, in error, that it does). We'll try with a device array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not(!) overwrite device array\n",
    "A_d = cuda.to_device(A)\n",
    "sum_reduce(A_d, res=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we check if `A_d` remains the same as it was when it was copied from the host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.allclose(A, A_d.copy_to_host()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ForAll Generator\n",
    "\n",
    "The ForAll generator takes a kernel that operates elementwise on an input array and returns a preconfigured kernel ready to launch on an array of a given size. We'll try it out with an example borrowed from the internals of [cuDF](https://github.com/rapidsai/cudf), which rounds elements to a given number of decimal places.\n",
    "\n",
    "First, we'll define our kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def rint(x):\n",
    "    \"\"\"Round to the nearest integer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The nearest integer, as a float.\n",
    "    \"\"\"\n",
    "    y = floor(x)\n",
    "    r = x - y \n",
    "\n",
    "    if r > 0.5:\n",
    "        y += 1.0 \n",
    "    if r == 0.5:\n",
    "        r = y - 2.0 * floor(0.5 * y)\n",
    "        if r == 1.0:\n",
    "            y += 1.0 \n",
    "    return y\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_round(in_col, out_col, decimal):\n",
    "    i = cuda.grid(1)\n",
    "    f = 10 ** decimal\n",
    "\n",
    "    if i < in_col.size:\n",
    "        ret = in_col[i] * f\n",
    "        ret = rint(ret)\n",
    "        tmp = ret / f\n",
    "        out_col[i] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll prepare some data - an input and output array, and note the number of elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_data and out_data are device arrays\n",
    "in_data = cuda.to_device(np.random.random(10))\n",
    "out_data = cuda.device_array_like(in_data)\n",
    "nelems = len(in_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the kernel to all elements of the array without having to work out an efficient launch configuration we call the `forall` method of the kernel, then call the result with the usual kernel arguments. The `forall` method calculates occupancy and generates an efficient launch configuration automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round all elements to 3 d.p.\n",
    "gpu_round.forall(nelems)(in_data, out_data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the input data and results to check that the kernel did what we expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_data.copy_to_host())\n",
    "print(out_data.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "### Caching\n",
    "\n",
    "During the first execution of a jitted function, Numba steps in and compiles the Python code, whereas subsequent executions retrieve a compiled version from a cache. This compilation time can be quite long in comparison to a single function/kernel call. When measuring the execution time of jitted functions, either on the CPU or GPU, it is important to avoid timing the compilation as well as the execution time.\n",
    "\n",
    "Timing the compilation can be avoided by only timing the second or later call to a jitted function. We'll see the difference in times with a small example kernel. The following defines the kernel, and sets up some data to call it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def go_fast(a):\n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):\n",
    "        trace += np.tanh(a[i, i])\n",
    "    return a + trace\n",
    "\n",
    "\n",
    "x = np.arange(4096).astype(np.float64).reshape((64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we time the first execution, we will get a measurement of the compilation time and the first execution time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation time included in the first execution\n",
    "start = time.time()\n",
    "go_fast(x)\n",
    "end = time.time()\n",
    "print(\"Elapsed (with compilation) = %sms\" % ((end - start) * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subsequent execution should require much less time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation time included in the first execution\n",
    "start = time.time()\n",
    "go_fast(x)\n",
    "end = time.time()\n",
    "print(\"Elapsed (with compilation) = %sms\" % ((end - start) * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular example, the difference in timing is likely to be orders of magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CUDA Array Interface\n",
    "\n",
    "This section demonstrates:\n",
    "\n",
    "* How the CUDA Array Interface enables interoperability between different CUDA-supporting Python libraries from the user's perspective with two examples:\n",
    "  - Calling a Numba kernel on a CuPy array\n",
    "  - Using CuPy on a Numba device array\n",
    "* How to implement the CUDA Array Interface in a Python library, using a simple example that wraps the Runtime API.\n",
    "\n",
    "\n",
    "### Calling a Numba kernel on a CuPy array\n",
    "\n",
    "We'll define a simple CUDA kernel that adds two arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add(x, y, out):\n",
    "    start = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(start, x.shape[0], stride):\n",
    "        out[i] = x[i] + y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create some CuPy arrays for input data and space for an output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cupy.arange(10)\n",
    "b = a * 2\n",
    "out = cupy.zeros_like(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without needing to do anything else from the user perspective, we can call our Numba kernel on CuPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add[1, 32](a, b, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and get the correct result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a+b)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked because Numba used the CUDA Array Interface to access data at the time of the launch - seeing objects other than its own device arrays, it accessed the `__cuda_array_interface__` property of the arrays to obtain a pointer to the data and the shape and strides of the arrays.\n",
    "\n",
    "### Creating CuPy arrays from Numba device arrays without copying\n",
    "\n",
    "Now let's work in the other direction - we'll create a CuPy array view of a Numba device array, so that we can operate on it with CuPy functions. First let's create a Numba array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: numpy.ndarray\n",
    "x = np.arange(10)\n",
    "\n",
    "# type: numba.cuda.cudadrv.devicearray.DeviceNDArray\n",
    "x_numba = cuda.to_device(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a CuPy array and compute the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: cupy.ndarray - a view of x_numba's data\n",
    "x_cupy = cupy.asarray(x_numba)\n",
    "\n",
    "x_cupy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we mutate the Numba array on the device then computing the mean again will use the updated values, because `x_numba` and `x_cupy` share the underlying data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_one(x):\n",
    "    i = cuda.grid(1)\n",
    "    if i < len(x):\n",
    "        x[i] += 1\n",
    "\n",
    "# Add one to every element of the array\n",
    "add_one[1, 32](x_numba)\n",
    "\n",
    "x_cupy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a CUDA Array Interface implementation\n",
    "\n",
    "Implementing the CUDA Array Interface so a library can export its data for other CUDA-aware libraries to use requires the implementation of one property, `__cuda_array_interface__`, on the objects that hold on-device data. This property returns a dict containing information about the data. For full details of the specification, see the [CUDA Array Interface documentation](http://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html).\n",
    "\n",
    "We'll implement the interface for a simple class that allocates and frees on-device memory using the CUDA Runtime API via ctypes.\n",
    "\n",
    "First we'll set up our ctypes access to the Runtime API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get access to the CUDA Runtime API via ctypes\n",
    "\n",
    "from ctypes import CDLL, POINTER, byref, c_void_p, c_size_t\n",
    "\n",
    "cudart = CDLL('libcudart.so')\n",
    "\n",
    "cudaMalloc = cudart.cudaMalloc\n",
    "cudaMalloc.argtypes = [POINTER(c_void_p), c_size_t]\n",
    "\n",
    "cudaFree = cudart.cudaFree\n",
    "cudaFree.argtypes = [c_void_p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the `MyFloatArray` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class to provide the CUDA Array interface.\n",
    "#\n",
    "# Simple example for read-write contiguous data.\n",
    "\n",
    "FLOAT32_SIZE = 4\n",
    "\n",
    "class MyFloatArray:\n",
    "    \"\"\"An array of floats that allocates memory on the device when constructed,\n",
    "    and frees it when it is deleted.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self._size = size\n",
    "        self._typestr = 'f4'\n",
    "        self._ptr = c_void_p()\n",
    "\n",
    "        alloc_size = size * FLOAT32_SIZE\n",
    "        ret = cudaMalloc(byref(self._ptr), alloc_size)\n",
    "        if ret:\n",
    "            raise RuntimeError(f'Unexpected return code {ret} from cudaMalloc')\n",
    "\n",
    "    def __del__(self):\n",
    "        cudaFree(self._ptr)\n",
    "\n",
    "    @property\n",
    "    def __cuda_array_interface__(self):\n",
    "        return {\n",
    "            # Fill in\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* Fill in code to return the entries in the dict returned by `__cuda_array_interface__`\n",
    "\n",
    "#### Continuing\n",
    "\n",
    "Once you have completed filling in the dict entries we can create an instance of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelems = 32\n",
    "arr = MyFloatArray(nelems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For checking things, we can access the CUDA Array Interface manually and print various fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_cai = arr.__cuda_array_interface__\n",
    "\n",
    "cai_ptr = arr_cai['data'][0]\n",
    "print(f\"Pointer from CAI is 0x{cai_ptr:x}\")\n",
    "\n",
    "print(f\"Shape from CAI is {arr_cai['shape']}\")\n",
    "print(f\"Type string from CAI is '{arr_cai['typestr']}'\")\n",
    "print(f\"Version from CAI is {arr_cai['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything looks as expected, we can launch a kernel on our `MyFloatArray` object transparently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def initialize(x):\n",
    "    i = cuda.grid(1)\n",
    "    if i < len(x):\n",
    "        x[i] = 3.14\n",
    "\n",
    "\n",
    "initialize[1, 32](arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also copy the data to the host and print it out to check that it is expected. For simplicity in this example, we'll use the `as_cuda_array()` function, that takes any object and creates a Numba device array from it, so that we can copy the data and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cuda.as_cuda_array(arr).copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that the array is populated with the value `3.14` in every element.\n",
    "\n",
    "A more thorough / complete test might have used the Runtime API to copy the data to the host then print it, but this would require some extra work in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging\n",
    "\n",
    "Session 3 also covered debugging techniques. For exercises with cuda-memcheck and the CUDA simulator, see the other notebook, `2-Debug.ipynb`.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've looked at how to:\n",
    "\n",
    "* Determine what is compiling in Object Mode and why by using the `numba` command line tool to create HTML source code listings annotated with Numba type information,\n",
    "* Rewrite array operations unsupported in the CUDA target, both with loops and scalar code in kernels, and using CuPy functions,\n",
    "* How to launch asynchronous transfers and kernels, and how to measure their performance using events,\n",
    "* How to use the CUDA target's utility functions:\n",
    "  - The Random Number Generator,\n",
    "  - The Reduce Generator,\n",
    "  - The ForAll Generator,\n",
    "* How to time jitted functions and kernel launches without including the compilation time,\n",
    "* How to use the CUDA array interface:\n",
    "  - From the usage perspective, with CuPy and Numba data,\n",
    "  - and how to implement the CUDA Array Interface for an object holding on-device data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
